{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, n0, n1, n2):\n",
    "        self.n_0 = n0\n",
    "        self.n_1 = n1\n",
    "        self.n_2 = n2\n",
    "\n",
    "        self.reinitialize_weights = True\n",
    "        \n",
    "        # weights and biases initialized as placeholders.\n",
    "        # reinitialized later\n",
    "        self.weights_1 = np.zeros((self.n_0, self.n_1), dtype=int)\n",
    "        self.weights_2 = np.zeros((self.n_1, self.n_2), dtype=int)\n",
    "        self.biases_1 = np.zeros((1, self.n_1), dtype=int)\n",
    "        self.biases_2 = np.zeros((1, self.n_2), dtype=int)\n",
    "\n",
    "        self.activations_1 = np.zeros((1, self.n_1), dtype=int)\n",
    "        self.activations_2 = np.zeros((1, self.n_2), dtype=int)\n",
    "        self.sensitivities_1 = np.zeros((1, self.n_1), dtype=int)\n",
    "        self.sensitivities_2 = np.zeros((1, self.n_2), dtype=int)\n",
    "\n",
    "        self.hyper_params = HyperParams()\n",
    "\n",
    "        self.model_info = Info()\n",
    "\n",
    "\n",
    "class HyperParams:\n",
    "    def __init__(self, no_training_steps=800, alpha_sch=2, percentage=0.98):\n",
    "        self.alpha_list = [0.1, 0.2, 0.3]\n",
    "        self.zeta_list = [0.5, 1, 1.5]\n",
    "        self.x0_list = [0.5, 1, 1.5]\n",
    "        self.max_epochs = 700  # empirically chosen\n",
    "        self.tolerance = 0.05\n",
    "        self.training_steps = no_training_steps\n",
    "        self.learning_rate = self.alpha_list[1]\n",
    "        self.lr_scheduling_option = alpha_sch  # {0: no_sch, 1: step_sch, 2: per_sch}\n",
    "        self.lr_perc_decrease = percentage\n",
    "        self.zeta = self.zeta_list[1]\n",
    "        self.x0 = self.x0_list[1]\n",
    "        self.cost_fn = 0  # {0: quadratic, 1: cross-entropy}\n",
    "\n",
    "\n",
    "class Info:\n",
    "    def __init__(self, total_epochs=0, last_epoch_error=0.0, convergence=False):\n",
    "        self.total_epochs_req = total_epochs\n",
    "        self.last_epoch_error = last_epoch_error\n",
    "        self.converged = convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sheets = {}\n",
    "\n",
    "\n",
    "def update_sheet(writer, sheet_name, sheet_obj):\n",
    "    df_obj = {\n",
    "        'Model architecture': sheet_obj['model_arch_list'],\n",
    "        'Model weights': sheet_obj['model_weight_list'],\n",
    "        'Model biases': sheet_obj['model_bias_list'],\n",
    "        '# Training epochs': sheet_obj['total_epochs_req_list'],\n",
    "        'Learning Rate': sheet_obj['learning_rate_list'],\n",
    "        'Zeta': sheet_obj['zeta_list'],\n",
    "        'X0': sheet_obj['x0_list'],\n",
    "        'Cost Function': sheet_obj['cost_fn_list'],\n",
    "        'Last epoch error': sheet_obj['last_epoch_error_list'],\n",
    "        'Did converge?': sheet_obj['converged_list'],\n",
    "    }\n",
    "    df = pd.DataFrame(df_obj)\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "\n",
    "def save_data(sheet_name, model):\n",
    "    try:\n",
    "        sheet_obj = sheets[sheet_name]\n",
    "    except KeyError:\n",
    "        sheet_obj = {}\n",
    "    if not sheet_obj:\n",
    "        sheet_obj = {'model_arch_list': [], 'model_weight_list': [], 'model_bias_list': [],\n",
    "                     'total_epochs_req_list': [], 'learning_rate_list': [], 'zeta_list': [],\n",
    "                     'x0_list': [], 'cost_fn_list': [], 'last_epoch_error_list': [],\n",
    "                     'converged_list': []}\n",
    "    sheet_obj['model_arch_list'].append(\"[ \" + str(model.n_0) + \", \" + str(model.n_1) +\n",
    "                                        \", \" + str(model.n_2) + \"]\")\n",
    "    sheet_obj['model_weight_list'].append(\"Weights1 = \" + str(model.weights_1) +\n",
    "                                          \"\\nWeights2 = \" + str(model.weights_2))\n",
    "    sheet_obj['model_bias_list'].append(\"Biases1 = \" + str(model.biases_1) +\n",
    "                                        \"\\nBiases2 = \" + str(model.biases_2))\n",
    "    sheet_obj['total_epochs_req_list'].append(model.model_info.total_epochs_req)\n",
    "    sheet_obj['learning_rate_list'].append(model.hyper_params.learning_rate)\n",
    "    sheet_obj['zeta_list'].append(model.hyper_params.zeta)\n",
    "    sheet_obj['x0_list'].append(model.hyper_params.x0)\n",
    "    sheet_obj['cost_fn_list'].append(\"Quadratic\" if model.hyper_params.cost_fn == 0\n",
    "                                     else \"Cross-Entropy\")\n",
    "    sheet_obj['last_epoch_error_list'].append(model.model_info.last_epoch_error)\n",
    "    sheet_obj['converged_list'].append(\"Yes\" if model.model_info.converged else \"No\")\n",
    "    sheets[sheet_name] = sheet_obj\n",
    "\n",
    "\n",
    "def export_data():\n",
    "    print(\"Starting export\")\n",
    "    writer = pd.ExcelWriter('Results.xlsx', engine='xlsxwriter')\n",
    "    if not writer:\n",
    "        print(\"Error while opening writer. Exiting.\")\n",
    "        return\n",
    "    for sheet_name in sheets:\n",
    "        sheet_obj = sheets[sheet_name]\n",
    "        if not sheet_obj:\n",
    "            print(\"Skipping sheet - %s \" % sheet_name)\n",
    "            continue\n",
    "        print(\"Updating sheet - %s \" % sheet_name)\n",
    "        update_sheet(writer, sheet_name, sheet_obj)\n",
    "    writer.save()\n",
    "    print(\"Data exported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use - python main.py --steps <num_of_training_steps> --alpha <alpha> --asch <alpha_scheduling_option> --perc <f_perc_value>\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import getopt\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from model import Model, HyperParams\n",
    "from save import save_data, export_data\n",
    "\n",
    "export_to_excel = False\n",
    "\n",
    "\n",
    "def transfer_ftn(n_l, x0):\n",
    "    a_l = np.tanh(n_l / (2 * x0))\n",
    "    return a_l\n",
    "\n",
    "\n",
    "# we only save a_l NOT n_l if using bipolar sigmoid transfer function\n",
    "def derivative_transfer_ftn(a_l, x0):\n",
    "    derivative = ((1 + a_l) * (1 - a_l)) / (2 * x0)\n",
    "    return derivative\n",
    "\n",
    "\n",
    "def init_weights_biases(model):\n",
    "    if model.reinitialize_weights:\n",
    "        model.weights_1 = np.random.uniform(-1 * model.hyper_params.zeta,\n",
    "                                            model.hyper_params.zeta, model.weights_1.shape)\n",
    "\n",
    "        model.weights_2 = np.random.uniform(-1 * model.hyper_params.zeta,\n",
    "                                            model.hyper_params.zeta, model.weights_2.shape)\n",
    "\n",
    "        model.biases_1 = np.random.uniform(-1 * model.hyper_params.zeta,\n",
    "                                           model.hyper_params.zeta, model.biases_1.shape)\n",
    "\n",
    "        model.biases_2 = np.random.uniform(-1 * model.hyper_params.zeta,\n",
    "                                           model.hyper_params.zeta, model.biases_2.shape)\n",
    "\n",
    "    return [model.weights_1, model.weights_2], [model.biases_1, model.biases_2]\n",
    "\n",
    "\n",
    "def train_nn(x_train, y_train, model):\n",
    "    Q = len(x_train)\n",
    "    weight_list, bias_list = init_weights_biases(model)\n",
    "    weight_list_len = len(weight_list)\n",
    "    for epoch in range(model.hyper_params.max_epochs):\n",
    "        epoch_error = 0\n",
    "        for iteration in range(Q):\n",
    "            x = x_train[iteration]\n",
    "            y = y_train[iteration]\n",
    "            x = np.array(x).reshape((1, len(x)))\n",
    "            y = np.array(y).reshape((1, len(y)))\n",
    "\n",
    "            # Calculate activations for all layers\n",
    "            # don't need to save n_l if we are using bipolar sigmoid transfer function\n",
    "            a_l_list = [x]\n",
    "            for i in range(len(weight_list)):\n",
    "                n_l = np.matmul(a_l_list[-1], weight_list[i]) + bias_list[i]\n",
    "                a_l = transfer_ftn(n_l, model.hyper_params.x0)\n",
    "                a_l_list.append(a_l)\n",
    "\n",
    "            # calculating the error for this example\n",
    "            y_hat = a_l_list[-1]  # activation of the last layer\n",
    "            example_error = np.matmul(y_hat - y, (y_hat - y).T)\n",
    "            example_error = np.asscalar(example_error)\n",
    "            epoch_error = epoch_error + example_error\n",
    "\n",
    "            # Calculate sensitivities for last layer. Performs element-wise multiplication.\n",
    "            # quadratic cost function\n",
    "            if model.hyper_params.cost_fn == 0:\n",
    "                s_L = np.multiply((y_hat - y), derivative_transfer_ftn(y_hat, model.hyper_params.x0))\n",
    "            # cross entropy cost function\n",
    "            elif model.hyper_params.cost_fn == 1:\n",
    "                s_L = y_hat - y\n",
    "\n",
    "            # Calculate sensitivites for other layers\n",
    "            sensitivities_list = [s_L]\n",
    "\n",
    "            for l in range(weight_list_len - 1, 0, -1):\n",
    "                s_l = np.multiply(np.matmul(sensitivities_list[0], weight_list[l].T), \\\n",
    "                                  derivative_transfer_ftn(a_l_list[l], model.hyper_params.x0))\n",
    "                sensitivities_list.insert(0, s_l)\n",
    "\n",
    "            # Update weights and biases\n",
    "            for l in range(weight_list_len):\n",
    "                weight_list[l] = weight_list[l] - \\\n",
    "                                 (model.hyper_params.learning_rate *\n",
    "                                  np.matmul(a_l_list[l].T, sensitivities_list[l]))\n",
    "\n",
    "                bias_list[l] = bias_list[l] - \\\n",
    "                               (model.hyper_params.learning_rate * sensitivities_list[l])\n",
    "\n",
    "        # epoch error is not normalized (not divided by number of examples)\n",
    "        if epoch_error < model.hyper_params.tolerance:\n",
    "            break\n",
    "\n",
    "    num_training_epochs = epoch + 1\n",
    "    if num_training_epochs < model.hyper_params.max_epochs:\n",
    "        convergence = True\n",
    "    else:\n",
    "        convergence = False\n",
    "\n",
    "    update_model_info(model, weight_list, bias_list, num_training_epochs, epoch_error, convergence)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def update_model_info(model, weight_list, bias_list, num_training_epochs, epoch_error, convergence):\n",
    "    model.weights_1 = weight_list[0]\n",
    "    model.weights_2 = weight_list[1]\n",
    "    model.biases_1 = bias_list[0]\n",
    "    model.biases_2 = bias_list[1]\n",
    "    model.model_info.total_epochs_req = num_training_epochs\n",
    "    model.model_info.last_epoch_error = epoch_error\n",
    "    model.model_info.converged = convergence\n",
    "\n",
    "\n",
    "def extract_model_info(model, sheet_name, verbose=True):\n",
    "    if verbose:\n",
    "        print(\"--------------------------------------------------------------------------------\")\n",
    "        print(f\"Learning rate = {model.hyper_params.learning_rate} | \"\n",
    "              f\"Zeta = {model.hyper_params.zeta} | \"\n",
    "              f\"x0 = {model.hyper_params.x0}\")\n",
    "        print(f\"Convergence = {model.model_info.converged} | \"\n",
    "              f\"Training Epochs = {model.model_info.total_epochs_req} | \"\n",
    "              f\"Squared Error = {model.model_info.last_epoch_error}\")\n",
    "        print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    if export_to_excel:\n",
    "        save_data(sheet_name, model)\n",
    "\n",
    "\n",
    "def part_2a(x_train, y_train, model, sheet_name):\n",
    "    # TODO\n",
    "    # Look for patterns when do we get non-convergent results\n",
    "    # Try all 3X3X3=27 hyper parameter combinations of alpha, zeta and x0\n",
    "    num_convergence = 0\n",
    "    for alpha in model.hyper_params.alpha_list:\n",
    "        for zeta in model.hyper_params.zeta_list:\n",
    "            for x0 in model.hyper_params.x0_list:\n",
    "                model.hyper_params.learning_rate = alpha\n",
    "                model.hyper_params.zeta = zeta\n",
    "                model.hyper_params.x0 = x0\n",
    "                model = train_nn(x_train, y_train, model)\n",
    "                if model.model_info.converged:\n",
    "                    num_convergence += 1\n",
    "                extract_model_info(model, sheet_name, verbose=True)\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    print(f\"Number of convergent hyper parameter combinations = {num_convergence} (out of 27)\")\n",
    "\n",
    "\n",
    "def part_2b(x_train, y_train, cost_fn, sheet_name):\n",
    "    N1_list = [1, 2, 4, 6, 8, 10]\n",
    "    convergence_list = []\n",
    "    for i in range(len(N1_list)):\n",
    "        model = Model(2, N1_list[i], 1)\n",
    "        model.hyper_params.cost_fn = cost_fn\n",
    "        num_convergence = 0\n",
    "        for iters in range(100):\n",
    "            model = train_nn(x_train, y_train, model)\n",
    "            if model.model_info.converged:\n",
    "                num_convergence += 1\n",
    "            extract_model_info(model, sheet_name, verbose=False)\n",
    "        convergence_list.append(num_convergence)\n",
    "        print(f\"Convergence for N1 = %d -> %d\" % (N1_list[i], num_convergence))\n",
    "\n",
    "    print(f\"Convergence results for N1 = [1,2,4,6,8,10] (out of 100): {convergence_list}\")\n",
    "\n",
    "    # Results mostly converge for N1=4 and above. For N1=2, almost 70% of the times,\n",
    "    # it converges. For N1=1, it doesn't converge at all.\n",
    "    # This is probably because the XOR problem is not linearly separable and we need a higher\n",
    "    # number of neurons in the hidden layer to approximate the function (see universality theorem).\n",
    "\n",
    "\n",
    "def xor_weight_validation(x_train, y_train, model, sheet_name):\n",
    "    model.hyper_params.max_epochs = 1\n",
    "\n",
    "    # Setting initial weights and biases for xor weight validation\n",
    "    model.weights_1 = np.array([[0.197, 0.3191, -0.1448, 0.3594],\n",
    "                                [0.3099, 0.1904, -0.0347, -0.4861]]).reshape(model.weights_1.shape)\n",
    "    model.weights_2 = np.array([0.4919, -0.2913, -0.3979, 0.3581]).reshape(model.weights_2.shape)\n",
    "    model.biases_1 = np.array([-0.3378, 0.2771, 0.2859, -0.3329]).reshape(model.biases_1.shape)\n",
    "    model.biases_2 = np.array([-0.1401]).reshape(model.biases_2.shape)\n",
    "    model.reinitialize_weights = False\n",
    "\n",
    "    model = train_nn(x_train, y_train, model)\n",
    "\n",
    "    print(\"W1=\", model.weights_1, sep=\"\\n\")\n",
    "    print(\"b1=\", model.biases_1, sep=\"\\n\")\n",
    "    print(\"W2=\", model.weights_2, sep=\"\\n\")\n",
    "    print(\"b2=\", model.biases_2, sep=\"\\n\")\n",
    "\n",
    "    extract_model_info(model, sheet_name, verbose=False)\n",
    "\n",
    "    # np.savez('xor_weight_validation.npz', model=model)\n",
    "    # results can be loaded from the xor_weight_validation.npz file by uncommenting the following\n",
    "    # data = np.load('Part1_results.npz')\n",
    "    # model = data['model']\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv, \"\", [\"steps=\", \"alpha=\", \"asch=\", \"perc=\"])\n",
    "    except getopt.GetoptError:\n",
    "        print(\"Use - python main.py --steps <num_of_training_steps> --alpha <alpha> \"\n",
    "              \"--asch <alpha_scheduling_option> --perc <f_perc_value>\")\n",
    "        sys.exit(2)\n",
    "    hyper_params = HyperParams()\n",
    "    for opt, arg in opts:\n",
    "        if opt == \"--steps\":\n",
    "            hyper_params.training_steps = int(arg)\n",
    "        elif opt == \"--alpha\":\n",
    "            hyper_params.learning_rate = float(arg)\n",
    "        elif opt == \"--asch\":\n",
    "            hyper_params.lr_scheduling_option = int(arg)\n",
    "        elif opt == \"--perc\":\n",
    "            hyper_params.lr_perc_decrease = float(arg)\n",
    "\n",
    "    # # uncomment this if data needs to be stored in excel\n",
    "    # global export_to_excel\n",
    "    # export_to_excel = True\n",
    "\n",
    "    x_train = [[1, 1], [1, -1], [-1, 1], [-1, -1]]\n",
    "    y_train = [[-1], [1], [1], [-1]]\n",
    "\n",
    "    model = Model(2, 4, 1)\n",
    "    model.hyper_params = hyper_params\n",
    "    xor_weight_validation(x_train, y_train, model, sheet_name=\"XOR weights validation\")\n",
    "\n",
    "    # using quadratic cost ftn\n",
    "    model = Model(2, 4, 1)\n",
    "    model.hyper_params.cost_fn = 0\n",
    "    part_2a(x_train, y_train, model, sheet_name=\"A-Z-X0 variations (Quad)\")\n",
    "    part_2b(x_train, y_train, cost_fn=0, sheet_name=\"N1 variations (Quad)\")\n",
    "\n",
    "    # using cross entropy cost ftn\n",
    "    model = Model(2, 4, 1)\n",
    "    model.hyper_params.cost_fn = 1\n",
    "    part_2a(x_train, y_train, model, sheet_name=\"A-Z-X0 variations (CrsEnt)\")\n",
    "    part_2b(x_train, y_train, cost_fn=1, sheet_name=\"N1 variations (CrsEnt)\")\n",
    "\n",
    "    model = Model(2, 4, 1)\n",
    "    model.hyper_params.learning_rate = 0.2\n",
    "    model.hyper_params.zeta = 1.0\n",
    "    model.hyper_params.x0 = 1.0\n",
    "    model.hyper_params.cost_fn = 1\n",
    "    model.hyper_params.max_epochs = 1\n",
    "    model = train_nn(x_train, y_train, model)\n",
    "    extract_model_info(model, sheet_name=\"Final verification\")\n",
    "\n",
    "    # should be set to true above\n",
    "    if export_to_excel:\n",
    "        export_data()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
