{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note (IMPORTANT! Please Read!)\n",
      " 1. The results generated by part2&3 be slightly different from the result \n",
      " included in the report due to random initialization of weight&bias for each run.\n",
      " 2. For Part 4, we use the weight/bias provided for part1\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# CS6673 Neural Network Computing\n",
    "# Final Project\n",
    "# -------------------------------------------\n",
    "# Note (IMPORTANT! Please Read!)\n",
    "# 1. The results generated by part2&3 be slightly different from the \n",
    "# result included in the report due to random initialization of weight&bias\n",
    "# before training.\n",
    "# 2. For part 4, we use the weight/bias provided for part1\n",
    "#\n",
    "#\n",
    "print(f\"Note (IMPORTANT! Please Read!)\\n 1. The results generated by part2&3 be slightly different from the result \\n included in the report due to random initialization of weight&bias for each run.\\n 2. For Part 4, we use the weight/bias provided for part1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, n0, n1, n2):\n",
    "        self.n_0 = n0\n",
    "        self.n_1 = n1\n",
    "        self.n_2 = n2\n",
    "\n",
    "        self.reinitialize_weights = True\n",
    "        \n",
    "        # weights and biases initialized as placeholders.\n",
    "        # reinitialized later\n",
    "        self.weights_1 = np.zeros((self.n_0, self.n_1), dtype=int)\n",
    "        self.weights_2 = np.zeros((self.n_1, self.n_2), dtype=int)\n",
    "        self.biases_1 = np.zeros((1, self.n_1), dtype=int)\n",
    "        self.biases_2 = np.zeros((1, self.n_2), dtype=int)\n",
    "\n",
    "        self.activations_1 = np.zeros((1, self.n_1), dtype=int)\n",
    "        self.activations_2 = np.zeros((1, self.n_2), dtype=int)\n",
    "        self.sensitivities_1 = np.zeros((1, self.n_1), dtype=int)\n",
    "        self.sensitivities_2 = np.zeros((1, self.n_2), dtype=int)\n",
    "\n",
    "        self.hyper_params = HyperParams()\n",
    "\n",
    "        self.model_info = Info()\n",
    "\n",
    "\n",
    "class HyperParams:\n",
    "    def __init__(self, no_training_steps=800, alpha_sch=2, percentage=0.98):\n",
    "        self.alpha_list = [0.1, 0.2, 0.3]\n",
    "        self.zeta_list = [0.5, 1, 1.5]\n",
    "        self.x0_list = [0.5, 1, 1.5]\n",
    "        self.max_epochs = 700  # empirically chosen\n",
    "        self.tolerance = 0.05\n",
    "        self.training_steps = no_training_steps\n",
    "        self.learning_rate = self.alpha_list[1]\n",
    "        self.lr_scheduling_option = alpha_sch  # {0: no_sch, 1: step_sch, 2: per_sch}\n",
    "        self.lr_perc_decrease = percentage\n",
    "        self.zeta = self.zeta_list[1]\n",
    "        self.x0 = self.x0_list[1]\n",
    "        self.cost_fn = 0  # {0: quadratic, 1: cross-entropy}\n",
    "\n",
    "class Info:\n",
    "    def __init__(self, total_epochs=0, last_epoch_error=0.0, convergence=False):\n",
    "        self.total_epochs_req = total_epochs\n",
    "        self.last_epoch_error = last_epoch_error\n",
    "        self.converged = convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "sheets = {}\n",
    "\n",
    "def update_sheet(writer, sheet_name, sheet_obj):\n",
    "    df_obj = {\n",
    "        'Model architecture': sheet_obj['model_arch_list'],\n",
    "        'Model weights': sheet_obj['model_weight_list'],\n",
    "        'Model biases': sheet_obj['model_bias_list'],\n",
    "        '# Training epochs': sheet_obj['total_epochs_req_list'],\n",
    "        'Learning Rate': sheet_obj['learning_rate_list'],\n",
    "        'Zeta': sheet_obj['zeta_list'],\n",
    "        'X0': sheet_obj['x0_list'],\n",
    "        'Cost Function': sheet_obj['cost_fn_list'],\n",
    "        'Last epoch error': sheet_obj['last_epoch_error_list'],\n",
    "        'Did converge?': sheet_obj['converged_list'],\n",
    "    }\n",
    "    df = pd.DataFrame(df_obj)\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "def save_data(sheet_name, model):\n",
    "    try:\n",
    "        sheet_obj = sheets[sheet_name]\n",
    "    except KeyError:\n",
    "        sheet_obj = {}\n",
    "    if not sheet_obj:\n",
    "        sheet_obj = {'model_arch_list': [], 'model_weight_list': [], 'model_bias_list': [],\n",
    "                     'total_epochs_req_list': [], 'learning_rate_list': [], 'zeta_list': [],\n",
    "                     'x0_list': [], 'cost_fn_list': [], 'last_epoch_error_list': [],\n",
    "                     'converged_list': []}\n",
    "    sheet_obj['model_arch_list'].append(\"[ \" + str(model.n_0) + \", \" + str(model.n_1) +\n",
    "                                        \", \" + str(model.n_2) + \"]\")\n",
    "    sheet_obj['model_weight_list'].append(\"Weights1 = \" + str(model.weights_1) +\n",
    "                                          \"\\nWeights2 = \" + str(model.weights_2))\n",
    "    sheet_obj['model_bias_list'].append(\"Biases1 = \" + str(model.biases_1) +\n",
    "                                        \"\\nBiases2 = \" + str(model.biases_2))\n",
    "    sheet_obj['total_epochs_req_list'].append(model.model_info.total_epochs_req)\n",
    "    sheet_obj['learning_rate_list'].append(model.hyper_params.learning_rate)\n",
    "    sheet_obj['zeta_list'].append(model.hyper_params.zeta)\n",
    "    sheet_obj['x0_list'].append(model.hyper_params.x0)\n",
    "    sheet_obj['cost_fn_list'].append(\"Quadratic\" if model.hyper_params.cost_fn == 0\n",
    "                                     else \"Cross-Entropy\")\n",
    "    sheet_obj['last_epoch_error_list'].append(model.model_info.last_epoch_error)\n",
    "    sheet_obj['converged_list'].append(\"Yes\" if model.model_info.converged else \"No\")\n",
    "    sheets[sheet_name] = sheet_obj\n",
    "\n",
    "def export_data():\n",
    "    print(\"Starting export\")\n",
    "    writer = pd.ExcelWriter('Results.xlsx', engine='xlsxwriter')\n",
    "    if not writer:\n",
    "        print(\"Error while opening writer. Exiting.\")\n",
    "        return\n",
    "    for sheet_name in sheets:\n",
    "        sheet_obj = sheets[sheet_name]\n",
    "        if not sheet_obj:\n",
    "            print(\"Skipping sheet - %s \" % sheet_name)\n",
    "            continue\n",
    "        print(\"Updating sheet - %s \" % sheet_name)\n",
    "        update_sheet(writer, sheet_name, sheet_obj)\n",
    "    writer.save()\n",
    "    print(\"Data exported\")\n",
    "\n",
    "def print_table_a(sheet_name):\n",
    "    global sheets\n",
    "    try:\n",
    "        sheet_obj = sheets[sheet_name]\n",
    "        simple_table = {\n",
    "            'α': sheet_obj['learning_rate_list'],\n",
    "            'ζ': sheet_obj['zeta_list'],\n",
    "            'X\\u2080': sheet_obj['x0_list'],\n",
    "            'Final Epoch Error': sheet_obj['last_epoch_error_list'],\n",
    "            'Convergence': sheet_obj['converged_list'],\n",
    "            '# Training Epochs': sheet_obj['total_epochs_req_list']\n",
    "        }\n",
    "        df = pd.DataFrame(simple_table)\n",
    "        print(df)\n",
    "    except KeyError:\n",
    "        print(\"Sheet doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_ftn(n_l, x0):\n",
    "    a_l = np.tanh(n_l / (2 * x0))\n",
    "    return a_l\n",
    "\n",
    "# we only save a_l NOT n_l if using bipolar sigmoid transfer function\n",
    "def derivative_transfer_ftn(a_l, x0):\n",
    "    derivative = ((1 + a_l) * (1 - a_l)) / (2 * x0)\n",
    "    return derivative\n",
    "\n",
    "def init_weights_biases(model):\n",
    "    if model.reinitialize_weights:\n",
    "        model.weights_1 = np.random.uniform(-1 * model.hyper_params.zeta,\n",
    "                                            model.hyper_params.zeta, model.weights_1.shape)\n",
    "        model.weights_2 = np.random.uniform(-1 * model.hyper_params.zeta,\n",
    "                                            model.hyper_params.zeta, model.weights_2.shape)\n",
    "        model.biases_1 = np.random.uniform(-1 * model.hyper_params.zeta,\n",
    "                                           model.hyper_params.zeta, model.biases_1.shape)\n",
    "        model.biases_2 = np.random.uniform(-1 * model.hyper_params.zeta,\n",
    "                                           model.hyper_params.zeta, model.biases_2.shape)\n",
    "    return [model.weights_1, model.weights_2], [model.biases_1, model.biases_2]\n",
    "\n",
    "def train_nn(x_train, y_train, model):\n",
    "    Q = len(x_train)\n",
    "    weight_list, bias_list = init_weights_biases(model)\n",
    "    weight_list_len = len(weight_list)\n",
    "    for epoch in range(model.hyper_params.max_epochs):\n",
    "        epoch_error = 0\n",
    "        for iteration in range(Q):\n",
    "            x = x_train[iteration]\n",
    "            y = y_train[iteration]\n",
    "            x = np.array(x).reshape((1, len(x)))\n",
    "            y = np.array(y).reshape((1, len(y)))\n",
    "\n",
    "            # Calculate activations for all layers\n",
    "            # don't need to save n_l if we are using bipolar sigmoid transfer function\n",
    "            a_l_list = [x]\n",
    "            for i in range(len(weight_list)):\n",
    "                n_l = np.matmul(a_l_list[-1], weight_list[i]) + bias_list[i]\n",
    "                a_l = transfer_ftn(n_l, model.hyper_params.x0)\n",
    "                a_l_list.append(a_l)\n",
    "\n",
    "            # calculating the error for this example\n",
    "            y_hat = a_l_list[-1]  # activation of the last layer\n",
    "            example_error = np.matmul(y_hat - y, (y_hat - y).T)\n",
    "            example_error = np.asscalar(example_error)\n",
    "            epoch_error = epoch_error + example_error\n",
    "\n",
    "            # Calculate sensitivities for last layer. Performs element-wise multiplication.\n",
    "            # quadratic cost function\n",
    "            if model.hyper_params.cost_fn == 0:\n",
    "                s_L = np.multiply((y_hat - y), derivative_transfer_ftn(y_hat, model.hyper_params.x0))\n",
    "            # cross entropy cost function\n",
    "            elif model.hyper_params.cost_fn == 1:\n",
    "                s_L = y_hat - y\n",
    "\n",
    "            # Calculate sensitivites for other layers\n",
    "            sensitivities_list = [s_L]\n",
    "\n",
    "            for l in range(weight_list_len - 1, 0, -1):\n",
    "                s_l = np.multiply(np.matmul(sensitivities_list[0], weight_list[l].T), \\\n",
    "                                  derivative_transfer_ftn(a_l_list[l], model.hyper_params.x0))\n",
    "                sensitivities_list.insert(0, s_l)\n",
    "\n",
    "            # Update weights and biases\n",
    "            for l in range(weight_list_len):\n",
    "                weight_list[l] = weight_list[l] - \\\n",
    "                                 (model.hyper_params.learning_rate *\n",
    "                                  np.matmul(a_l_list[l].T, sensitivities_list[l]))\n",
    "\n",
    "                bias_list[l] = bias_list[l] - \\\n",
    "                               (model.hyper_params.learning_rate * sensitivities_list[l])\n",
    "\n",
    "        # epoch error is not normalized (not divided by number of examples)\n",
    "        if epoch_error < model.hyper_params.tolerance:\n",
    "            break\n",
    "\n",
    "    num_training_epochs = epoch + 1\n",
    "    if num_training_epochs < model.hyper_params.max_epochs:\n",
    "        convergence = True\n",
    "    else:\n",
    "        convergence = False\n",
    "    update_model_info(model, weight_list, bias_list, num_training_epochs, epoch_error, convergence)\n",
    "    return model\n",
    "\n",
    "def update_model_info(model, weight_list, bias_list, num_training_epochs, epoch_error, convergence):\n",
    "    model.weights_1 = weight_list[0]\n",
    "    model.weights_2 = weight_list[1]\n",
    "    model.biases_1 = bias_list[0]\n",
    "    model.biases_2 = bias_list[1]\n",
    "    model.model_info.total_epochs_req = num_training_epochs\n",
    "    model.model_info.last_epoch_error = epoch_error\n",
    "    model.model_info.converged = convergence\n",
    "\n",
    "def extract_model_info(model, sheet_name, verbose=False, export_to_excel=True):\n",
    "    if verbose:\n",
    "        print(\"--------------------------------------------------------------------------------\")\n",
    "        print(f\"Learning rate = {model.hyper_params.learning_rate} | \"\n",
    "              f\"Zeta = {model.hyper_params.zeta} | \"\n",
    "              f\"x0 = {model.hyper_params.x0}\")\n",
    "        print(f\"Convergence = {model.model_info.converged} | \"\n",
    "              f\"Training Epochs = {model.model_info.total_epochs_req} | \"\n",
    "              f\"Squared Error = {model.model_info.last_epoch_error}\")\n",
    "        print(\"--------------------------------------------------------------------------------\")\n",
    "    elif export_to_excel:\n",
    "        save_data(sheet_name, model)\n",
    "\n",
    "def part_2a(x_train, y_train, model, sheet_name, table=None):\n",
    "    # TODO\n",
    "    # Look for patterns when do we get non-convergent results\n",
    "    # Try all 3X3X3=27 hyper parameter combinations of alpha, zeta and x0\n",
    "    num_convergence = 0\n",
    "    for alpha in model.hyper_params.alpha_list:\n",
    "        for zeta in model.hyper_params.zeta_list:\n",
    "            for x0 in model.hyper_params.x0_list:\n",
    "                model.hyper_params.learning_rate = alpha\n",
    "                model.hyper_params.zeta = zeta\n",
    "                model.hyper_params.x0 = x0\n",
    "                model = train_nn(x_train, y_train, model)\n",
    "                if model.model_info.converged:\n",
    "                    num_convergence += 1\n",
    "                # extract_model_info(model, sheet_name, verbose=True)\n",
    "                extract_model_info(model, sheet_name, verbose=False) # for demo purpose\n",
    "    print(\"-----------------------------------------------------------------------\\n\")\n",
    "    print(f\"Number of convergent hyper parameter combinations = {num_convergence} (out of 27)\")\n",
    "    print(\"-----------------------------------------------------------------------\\n\")\n",
    "\n",
    "def part_2b(x_train, y_train, cost_fn, sheet_name, table=None):\n",
    "    N1_list = [1, 2, 4, 6, 8, 10]\n",
    "    convergence_list = []\n",
    "    for i in range(len(N1_list)):\n",
    "        model = Model(2, N1_list[i], 1)\n",
    "        model.hyper_params.cost_fn = cost_fn\n",
    "        num_convergence = 0\n",
    "        for iters in range(100):\n",
    "            model = train_nn(x_train, y_train, model)\n",
    "            if model.model_info.converged:\n",
    "                num_convergence += 1\n",
    "            extract_model_info(model, sheet_name, verbose=False)\n",
    "        convergence_list.append(num_convergence)\n",
    "        print(f\"Convergence for N1 = %d -> %d\" % (N1_list[i], num_convergence))\n",
    "\n",
    "    print(f\"Convergence results for N1 = [1,2,4,6,8,10] (out of 100): {convergence_list}\")\n",
    "\n",
    "    # Results mostly converge for N1=4 and above. For N1=2, almost 70% of the times,\n",
    "    # it converges. For N1=1, it doesn't converge at all.\n",
    "    # This is probably because the XOR problem is not linearly separable and we need a higher\n",
    "    # number of neurons in the hidden layer to approximate the function (see universality theorem).\n",
    "\n",
    "def xor_weight_validation(x_train, y_train, model, sheet_name):\n",
    "    model.hyper_params.max_epochs = 1\n",
    "\n",
    "    # Setting initial weights and biases for xor weight validation\n",
    "    model.weights_1 = np.array([[0.197, 0.3191, -0.1448, 0.3594],\n",
    "                                [0.3099, 0.1904, -0.0347, -0.4861]]).reshape(model.weights_1.shape)\n",
    "    model.weights_2 = np.array([0.4919, -0.2913, -0.3979, 0.3581]).reshape(model.weights_2.shape)\n",
    "    model.biases_1 = np.array([-0.3378, 0.2771, 0.2859, -0.3329]).reshape(model.biases_1.shape)\n",
    "    model.biases_2 = np.array([-0.1401]).reshape(model.biases_2.shape)\n",
    "    model.reinitialize_weights = False\n",
    "\n",
    "    model = train_nn(x_train, y_train, model)\n",
    "\n",
    "    print(\"Weights1=\", model.weights_1, sep=\"\\n\")\n",
    "    print(\"Biases1=\", model.biases_1, sep=\"\\n\")\n",
    "    print(\"Weights2=\", model.weights_2, sep=\"\\n\")\n",
    "    print(\"Biases2=\", model.biases_2, sep=\"\\n\")\n",
    "\n",
    "def final_verification(x_train, y_train, model):\n",
    "    model.hyper_params.learning_rate = 0.2\n",
    "    model.hyper_params.zeta = 1.0\n",
    "    model.hyper_params.x0 = 1.0\n",
    "    model.hyper_params.cost_fn = 1\n",
    "    model.hyper_params.max_epochs = 1\n",
    "    model.weights_1 = np.array([[0.197, 0.3191, -0.1448, 0.3594],\n",
    "                                [0.3099, 0.1904, -0.0347, -0.4861]]).reshape(model.weights_1.shape)\n",
    "    model.weights_2 = np.array([0.4919, -0.2913, -0.3979, 0.3581]).reshape(model.weights_2.shape)\n",
    "    model.biases_1 = np.array([-0.3378, 0.2771, 0.2859, -0.3329]).reshape(model.biases_1.shape)\n",
    "    model.biases_2 = np.array([-0.1401]).reshape(model.biases_2.shape)\n",
    "    model.reinitialize_weights = False\n",
    "    model = train_nn(x_train, y_train, model)\n",
    "    print(\"Weights1=\", model.weights_1, sep=\"\\n\")\n",
    "    print(\"Biases1=\", model.biases_1, sep=\"\\n\")\n",
    "    print(\"Weights2=\", model.weights_2, sep=\"\\n\")\n",
    "    print(\"Biases2=\", model.biases_2, sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Results Demo\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights1=\n",
      "[[ 0.19347555  0.31675476 -0.144748    0.36374521]\n",
      " [ 0.30686735  0.18845288 -0.03301594 -0.48859019]]\n",
      "Biases1=\n",
      "[[-0.32243413  0.26504268  0.27330512 -0.32503622]]\n",
      "Weights2=\n",
      "[[ 0.47534885]\n",
      " [-0.27642811]\n",
      " [-0.38395025]\n",
      " [ 0.34801327]]\n",
      "Biases2=\n",
      "[[-0.08027444]]\n"
     ]
    }
   ],
   "source": [
    "# Part 1: XOR Weights Validation\n",
    "x_train = [[1, 1], [1, -1], [-1, 1], [-1, -1]]\n",
    "y_train = [[-1], [1], [1], [-1]]\n",
    "model = Model(2, 4, 1)\n",
    "model.hyper_params = HyperParams()\n",
    "xor_weight_validation(x_train, y_train, model, sheet_name=\"XOR weights validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "\n",
      "Number of convergent hyper parameter combinations = 16 (out of 27)\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "      α    ζ   X₀  Final Epoch Error Convergence  # Training Epochs\n",
      "0   0.1  0.5  0.5           0.049993         Yes                151\n",
      "1   0.1  0.5  1.0           0.406422          No                700\n",
      "2   0.1  0.5  1.5           4.049384          No                700\n",
      "3   0.1  1.0  0.5           0.049455         Yes                110\n",
      "4   0.1  1.0  1.0           0.049870         Yes                485\n",
      "5   0.1  1.0  1.5           3.924008          No                700\n",
      "6   0.1  1.5  0.5           3.996796          No                700\n",
      "7   0.1  1.5  1.0           0.049971         Yes                588\n",
      "8   0.1  1.5  1.5           0.062027          No                700\n",
      "9   0.2  0.5  0.5           0.049036         Yes                 64\n",
      "10  0.2  0.5  1.0           4.204976          No                700\n",
      "11  0.2  0.5  1.5           4.093781          No                700\n",
      "12  0.2  1.0  0.5           0.049674         Yes                 54\n",
      "13  0.2  1.0  1.0           0.049949         Yes                236\n",
      "14  0.2  1.0  1.5           0.055472          No                700\n",
      "15  0.2  1.5  0.5           0.049167         Yes                555\n",
      "16  0.2  1.5  1.0           0.049913         Yes                248\n",
      "17  0.2  1.5  1.5           0.049957         Yes                611\n",
      "18  0.3  0.5  0.5           5.313238          No                700\n",
      "19  0.3  0.5  1.0           4.310545          No                700\n",
      "20  0.3  0.5  1.5           4.136845          No                700\n",
      "21  0.3  1.0  0.5           0.048860         Yes                 29\n",
      "22  0.3  1.0  1.0           0.049617         Yes                187\n",
      "23  0.3  1.0  1.5           0.049693         Yes                351\n",
      "24  0.3  1.5  0.5           0.049865         Yes                 43\n",
      "25  0.3  1.5  1.0           0.049675         Yes                104\n",
      "26  0.3  1.5  1.5           0.049973         Yes                442\n"
     ]
    }
   ],
   "source": [
    "# Part 2a: Verying alpha, zeta and x0 (Quadratic Cost Function)\n",
    "# using quadratic cost function\n",
    "\n",
    "model = Model(2, 4, 1)\n",
    "model.hyper_params.cost_fn = 0\n",
    "part_2a(x_train, y_train, model, sheet_name=\"A-Z-X0 variations (Quad)\")\n",
    "print_table_a(\"A-Z-X0 variations (Quad)\") # Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for N1 = 1 -> 0\n",
      "Convergence for N1 = 2 -> 84\n",
      "Convergence for N1 = 4 -> 99\n",
      "Convergence for N1 = 6 -> 100\n",
      "Convergence for N1 = 8 -> 100\n",
      "Convergence for N1 = 10 -> 98\n",
      "Convergence results for N1 = [1,2,4,6,8,10] (out of 100): [0, 84, 99, 100, 100, 98]\n"
     ]
    }
   ],
   "source": [
    "# part 2b: Varying N1 (hidden layer neurons) (Quadratic Cost Functions)\n",
    "part_2b(x_train, y_train, cost_fn=0, sheet_name=\"N1 variations (Quad)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "\n",
      "Number of convergent hyper parameter combinations = 16 (out of 27)\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "      α    ζ   X₀  Final Epoch Error Convergence  # Training Epochs\n",
      "0   0.1  0.5  0.5           0.049993         Yes                151\n",
      "1   0.1  0.5  1.0           0.406422          No                700\n",
      "2   0.1  0.5  1.5           4.049384          No                700\n",
      "3   0.1  1.0  0.5           0.049455         Yes                110\n",
      "4   0.1  1.0  1.0           0.049870         Yes                485\n",
      "5   0.1  1.0  1.5           3.924008          No                700\n",
      "6   0.1  1.5  0.5           3.996796          No                700\n",
      "7   0.1  1.5  1.0           0.049971         Yes                588\n",
      "8   0.1  1.5  1.5           0.062027          No                700\n",
      "9   0.2  0.5  0.5           0.049036         Yes                 64\n",
      "10  0.2  0.5  1.0           4.204976          No                700\n",
      "11  0.2  0.5  1.5           4.093781          No                700\n",
      "12  0.2  1.0  0.5           0.049674         Yes                 54\n",
      "13  0.2  1.0  1.0           0.049949         Yes                236\n",
      "14  0.2  1.0  1.5           0.055472          No                700\n",
      "15  0.2  1.5  0.5           0.049167         Yes                555\n",
      "16  0.2  1.5  1.0           0.049913         Yes                248\n",
      "17  0.2  1.5  1.5           0.049957         Yes                611\n",
      "18  0.3  0.5  0.5           5.313238          No                700\n",
      "19  0.3  0.5  1.0           4.310545          No                700\n",
      "20  0.3  0.5  1.5           4.136845          No                700\n",
      "21  0.3  1.0  0.5           0.048860         Yes                 29\n",
      "22  0.3  1.0  1.0           0.049617         Yes                187\n",
      "23  0.3  1.0  1.5           0.049693         Yes                351\n",
      "24  0.3  1.5  0.5           0.049865         Yes                 43\n",
      "25  0.3  1.5  1.0           0.049675         Yes                104\n",
      "26  0.3  1.5  1.5           0.049973         Yes                442\n"
     ]
    }
   ],
   "source": [
    "# Part 3a: Varying alpha, zeta and x0 (hidden layer neurons) (Cross Entropy Cost Function)\n",
    "# using cross entropy cost ftn\n",
    "model.hyper_params.cost_fn = 1\n",
    "part_2a(x_train, y_train, model, sheet_name=\"A-Z-X0 variations (CrsEnt)\")\n",
    "print_table_a(\"A-Z-X0 variations (Quad)\") # Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for N1 = 1 -> 0\n",
      "Convergence for N1 = 2 -> 61\n",
      "Convergence for N1 = 4 -> 96\n",
      "Convergence for N1 = 6 -> 98\n",
      "Convergence for N1 = 8 -> 99\n",
      "Convergence for N1 = 10 -> 100\n",
      "Convergence results for N1 = [1,2,4,6,8,10] (out of 100): [0, 61, 96, 98, 99, 100]\n"
     ]
    }
   ],
   "source": [
    "# Part 3b: Varying N1 (hidden layer neurons) (Cross Entropy Cost Function)\n",
    "part_2b(x_train, y_train, cost_fn=1, sheet_name=\"N1 variations (CrsEnt)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights1=\n",
      "[[ 0.19383967  0.30895515 -0.14727152  0.36911844]\n",
      " [ 0.29881712  0.18811518 -0.02889164 -0.48928638]]\n",
      "Biases1=\n",
      "[[-0.30754551  0.24693804  0.25732658 -0.30887916]]\n",
      "Weights2=\n",
      "[[ 0.44724602]\n",
      " [-0.24360234]\n",
      " [-0.35686098]\n",
      " [ 0.32501904]]\n",
      "Biases2=\n",
      "[[-0.01923564]]\n"
     ]
    }
   ],
   "source": [
    "# Part 4: Weights and biases for N1 = 4, alpha = 0.2, zeta = 1.0 and x0 = 1.0 after 1 epoch\n",
    "final_verification(x_train, y_train, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
